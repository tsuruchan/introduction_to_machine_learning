{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "MachineLeaningCourse vol.4\n",
    "# 正則化回帰"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "今回は、これまで勉強してきた回帰モデルに対して、正則化を適用することによって「過学習」を抑制する手法を説明していきます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 正則化とは？\n",
    "簡単に言うと、過学習を回避するためのテクニックです。\n",
    "\n",
    "例えば、モデルに５００個の特徴量を入れて学習したとき、５００個のパラメータがコスト関数を最小化させる解として計算されます。\n",
    "\n",
    "このときにパラメータが多すぎると、テストデータだけににフィットさせる為にたまたま現れてしまう__汎化性のないパターンを学習されてしまい、過学習の原因となります__。\n",
    "\n",
    "それの対処法としてあげられるのが、「__正則化__」です。\n",
    "\n",
    "$$Regularization = \\frac{1}{n} \\sum^{n}_{i=0} (y^{(i)}-\\hat{y}^{(i)})^2　+ f(\\beta)$$\n",
    "\n",
    "普通のモデルであれば、二乗誤差を最小化するだけですが、正則化を用いると一緒に $f(\\beta)$も最小化します。\n",
    "\n",
    "イメージがつきやすい言い方にすると、「__誤差と一緒にパラメーターの絶対値も最小化しよう__」という感じです。\n",
    "\n",
    "$f(\\beta)$に入る式によって、正則化回帰の種類が分かれます。\n",
    "- リッジ回帰\n",
    "- LASSO\n",
    "- ElasticNet\n",
    "\n",
    "の３つを今回は説明していきます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### リッジ回帰\n",
    "リッジ回帰はL2ペナルティ付きのモデルである。このモデルの式では、最小二乗コスト関数に対して__重みの二乗和__を足し合わせます。\n",
    "\n",
    "$$Ridge = \\sum^{n}_{i=0} (y^{(i)}-\\hat{y}^{(i)})^2　+ \\lambda\\sum^{m}_{j=1} w^2_j$$\n",
    "\n",
    "リッジ回帰は、式の通りで $w$の大きさに制限を加えています。\n",
    "\n",
    "モデル構築用データセットの目的変数の誤差の二乗和のみを小さくするのではなく、回帰係数の二乗和も一緒に小さくしています。\n",
    "\n",
    "つまり、\n",
    "\n",
    "\n",
    "> {誤差の二乗和}＋λ{回帰係数の二乗和}\n",
    "\n",
    "\n",
    "を最小化します。これによって、回帰係数が大きくなってモデルが複雑になることを防ぎます。\n",
    "\n",
    "最小化するときの、誤差の二乗和の項に対する回帰係数の二乗和の項の比率である$\\lambda$の値は解析者が決める必要があります。\n",
    "\n",
    "$\\lambda$の値を__大きく__すると、__正則化の強さを引き上げ、モデルの重みを小さくします__。\n",
    "\n",
    "\n",
    "リッジ回帰の効果をまとめると、__「L2ペナルティーにより係数を縮小して過学習を抑える」__となります。\n",
    "\n",
    "では、前回と同様にbostonデータセットで学習していきましょう。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tsuruoka/.pyenv/versions/anaconda3-2.4.0/lib/python3.5/site-packages/sklearn/utils/fixes.py:64: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() instead\n",
      "  if 'order' in inspect.getargspec(np.copy)[0]:\n"
     ]
    }
   ],
   "source": [
    "# データセットの読み込み\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "# データセットを読み込み\n",
    "housing = load_boston()\n",
    "columns = housing.feature_names\n",
    "\n",
    "# Pandasのデータフレームに変換\n",
    "boston = pd.DataFrame(housing.data, columns=columns)\n",
    "\n",
    "# 目的変数をDataFrameへ変換\n",
    "boston['MEDV'] = np.array(housing.target)\n",
    "columns = list(columns)\n",
    "columns.append('MEDV')\n",
    "\n",
    "# 説明変数\n",
    "X = boston.loc[:, housing.feature_names].values\n",
    "# 目的変数\n",
    "y = boston.loc[:, 'MEDV'].values\n",
    "\n",
    "# トレーニングデータ：７割、テストデータ：３割にデータを分割\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "それでは、scikit-learnのRidge回帰を行いましょう。\n",
    "\n",
    "【公式ドキュメント】\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tsuruoka/.pyenv/versions/anaconda3-2.4.0/lib/python3.5/site-packages/sklearn/base.py:175: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() instead\n",
      "  args, varargs, kw, default = inspect.getargspec(init)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
       "   normalize=False, solver='auto', tol=0.001)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "# ここでのalphaは数式のλと同じです\n",
    "model= Ridge(alpha=1.0)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2 Train : 0.762, Test : 0.667\n"
     ]
    }
   ],
   "source": [
    "print('R^2 Train : %.3f, Test : %.3f' % (model.score(X_train, y_train), model.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "alpha=1.0の場合は上記の様なスコアになりました。\n",
    "\n",
    "今度は、alphaの値を変えて最適な値を推定しましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha : 0.000010, R^2 Train : 0.764, Test : 0.674\n",
      "alpha : 0.000100, R^2 Train : 0.764, Test : 0.674\n",
      "alpha : 0.001000, R^2 Train : 0.764, Test : 0.674\n",
      "alpha : 0.010000, R^2 Train : 0.764, Test : 0.673\n",
      "alpha : 0.100000, R^2 Train : 0.764, Test : 0.673\n",
      "alpha : 1.000000, R^2 Train : 0.762, Test : 0.667\n",
      "alpha : 10.000000, R^2 Train : 0.756, Test : 0.657\n",
      "alpha : 100.000000, R^2 Train : 0.739, Test : 0.642\n",
      "alpha : 1000.000000, R^2 Train : 0.687, Test : 0.600\n",
      "alpha : 10000.000000, R^2 Train : 0.556, Test : 0.482\n"
     ]
    }
   ],
   "source": [
    "# alphaの数値を0.00001~10000まで変えてR^2を求める\n",
    "\n",
    "for i in range(-5, 5):\n",
    "    al = 10 ** i\n",
    "    model= Ridge(alpha=al)\n",
    "    model.fit(X_train, y_train)\n",
    "    print('alpha : %f, R^2 Train : %.3f, Test : %.3f' % (al, model.score(X_train, y_train), model.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最も良い結果をだしたのは、\n",
    "alpha :0.001, 0.0001, 0.00001での\n",
    "\n",
    "~~~\n",
    "R^2 Train : 0.764, Test : 0.674\n",
    "~~~\n",
    "という結果ですね。\n",
    "\n",
    "alpha : 1.0(default)の場合は\n",
    "\n",
    "~~~\n",
    "R^2 Train : 0.762, Test : 0.667\n",
    "~~~\n",
    "でしたので、少しスコアが改善しています。\n",
    "\n",
    "\n",
    "では、重み（ｗ）を見てみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "線形回帰の重み(w)\n",
      "[ -1.19858618e-01   4.44233009e-02   1.18612465e-02   2.51295058e+00\n",
      "  -1.62710374e+01   3.84909910e+00  -9.85471557e-03  -1.50002715e+00\n",
      "   2.41507916e-01  -1.10671867e-02  -1.01897720e+00   6.95273216e-03\n",
      "  -4.88110587e-01]\n",
      "37.9925927703\n",
      "\n",
      "Ridge回帰(alpha=0.001の重み(w)\n",
      "[ -1.19852241e-01   4.44264240e-02   1.17974944e-02   2.51295892e+00\n",
      "  -1.62553136e+01   3.84921593e+00  -9.87075198e-03  -1.49982374e+00\n",
      "   2.41458946e-01  -1.10682078e-02  -1.01880481e+00   6.95361177e-03\n",
      "  -4.88121821e-01]\n",
      "37.9816068082\n"
     ]
    }
   ],
   "source": [
    "# 重み\n",
    "from sklearn.linear_model import LinearRegression\n",
    "model= LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "print(\"線形回帰の重み(w)\")\n",
    "print(model.coef_)\n",
    "print(model.intercept_)\n",
    "print(\"\")\n",
    "model= Ridge(alpha=0.001)\n",
    "model.fit(X_train, y_train)\n",
    "print(\"Ridge回帰(alpha=0.001の重み(w)\")\n",
    "print(model.coef_)\n",
    "print(model.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bostonデータセットはデータ量・特徴量ともに少ない方なので正則化の恩恵が少ないようですね笑"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LASSO\n",
    "LASSOはL1ペナルティ付きのモデルである。このモデルの式では、モデル構築用データセットの目的変数の誤差の二乗和と一緒に、回帰係数の絶対値の和も小さくしています。\n",
    "\n",
    "$$LASSO = \\frac{1}{2n} \\sum^{n}_{i=0} (y^{(i)}-\\hat{y}^{(i)})^2　+ \\lambda\\sum^{m}_{j=1} |w_j|$$\n",
    "\n",
    "つまり、\n",
    "\n",
    "> {誤差の二乗和}＋λ{回帰係数の絶対値の和)}\n",
    "\n",
    "を最小化します。\n",
    "\n",
    "LASSOの効果をまとめると、__「L1ペナルティーにより変数選択と次元削減を行う」__となります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "それでは、scikit-learnのLASSOを実行していきましょう。\n",
    "\n",
    "【公式ドキュメント】\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tsuruoka/.pyenv/versions/anaconda3-2.4.0/lib/python3.5/site-packages/sklearn/base.py:175: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() instead\n",
      "  args, varargs, kw, default = inspect.getargspec(init)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Lasso(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
       "   normalize=False, positive=False, precompute=False, random_state=None,\n",
       "   selection='cyclic', tol=0.0001, warm_start=False)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "model= Lasso(alpha=1.0)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2 Train : 0.708, Test : 0.611\n"
     ]
    }
   ],
   "source": [
    "print('R^2 Train : %.3f, Test : %.3f' % (model.score(X_train, y_train), model.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "alpha=1.0の場合は上記の様なスコアになりました。\n",
    "\n",
    "リッジ回帰と同様に、alphaの値を変えて最適な値を推定しましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha : 0.000010, R^2 Train : 0.764, Test : 0.674\n",
      "alpha : 0.000100, R^2 Train : 0.764, Test : 0.674\n",
      "alpha : 0.001000, R^2 Train : 0.764, Test : 0.673\n",
      "alpha : 0.010000, R^2 Train : 0.764, Test : 0.671\n",
      "alpha : 0.100000, R^2 Train : 0.753, Test : 0.653\n",
      "alpha : 1.000000, R^2 Train : 0.708, Test : 0.611\n",
      "alpha : 10.000000, R^2 Train : 0.538, Test : 0.483\n",
      "alpha : 100.000000, R^2 Train : 0.235, Test : 0.196\n",
      "alpha : 1000.000000, R^2 Train : 0.000, Test : -0.006\n",
      "alpha : 10000.000000, R^2 Train : 0.000, Test : -0.006\n"
     ]
    }
   ],
   "source": [
    "# alphaの数値を0.00001~10000まで変えてR^2を求める\n",
    "\n",
    "for i in range(-5, 5):\n",
    "    al = 10 ** i\n",
    "    model= Lasso(alpha=al)\n",
    "    model.fit(X_train, y_train)\n",
    "    print('alpha : %f, R^2 Train : %.3f, Test : %.3f' % (al, model.score(X_train, y_train), model.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最も良い結果をだしたのは、\n",
    "alpha :0.0001, 0.00001での\n",
    "\n",
    "~~~\n",
    "R^2 Train : 0.764, Test : 0.674\n",
    "~~~\n",
    "という結果ですね。\n",
    "\n",
    "alpha : 1.0(default)の場合は\n",
    "\n",
    "~~~\n",
    "R^2 Train :　0.708, Test : 0.611\n",
    "~~~\n",
    "でしたので、少しスコアが改善しています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "線形回帰の重み(w)\n",
      "[ -1.19858618e-01   4.44233009e-02   1.18612465e-02   2.51295058e+00\n",
      "  -1.62710374e+01   3.84909910e+00  -9.85471557e-03  -1.50002715e+00\n",
      "   2.41507916e-01  -1.10671867e-02  -1.01897720e+00   6.95273216e-03\n",
      "  -4.88110587e-01]\n",
      "37.9925927703\n",
      "\n",
      "Ridge回帰(alpha=0.001の重み(w)\n",
      "[ -1.19843008e-01   4.44292496e-02   1.17197651e-02   2.51157621e+00\n",
      "  -1.62363271e+01   3.84913623e+00  -9.88538154e-03  -1.49953756e+00\n",
      "   2.41405637e-01  -1.10696768e-02  -1.01860377e+00   6.95484627e-03\n",
      "  -4.88155690e-01]\n",
      "37.9697218102\n"
     ]
    }
   ],
   "source": [
    "# 重み\n",
    "model= LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "print(\"線形回帰の重み(w)\")\n",
    "print(model.coef_)\n",
    "print(model.intercept_)\n",
    "print(\"\")\n",
    "\n",
    "model= Lasso(alpha=0.0001)\n",
    "model.fit(X_train, y_train)\n",
    "print(\"Ridge回帰(alpha=0.001の重み(w)\")\n",
    "print(model.coef_)\n",
    "print(model.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elastic Net\n",
    "ElasticNetは、リッジ回帰とLASSOの折衷案である。\n",
    "\n",
    "$$ElasticNet = \\frac{1}{2n} \\sum^{n}_{i=0} (y^{(i)}-\\hat{y}^{(i)})^2　+ \\lambda_1\\sum^{m}_{j=1} w^2_j  +  \\lambda_2\\sum^{m}_{j=1} |w_j|$$\n",
    "\n",
    "つまり、\n",
    "\n",
    "\n",
    "> {誤差の二乗和}＋λ1{回帰係数の二乗和}＋λ2{回帰係数の絶対値の和}\n",
    "\n",
    "となる。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "それでは、scikit-learnのElasticNetを行いましょう。\n",
    "\n",
    "scikir-learnでは以下の式を最小化する。\n",
    "\n",
    "$$ElasticNet = \\frac{1}{2n} \\sum^{n}_{i=0} (y^{(i)}-\\hat{y}^{(i)})^2　+ \\alpha *L_1ratio\\sum^{m}_{j=1} |w_j|  +  \\frac{1}{2} \\alpha* (1 - L_1ratio)\\sum^{m}_{j=1} w^2_j$$\n",
    "\n",
    "【公式ドキュメント】\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNet.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ElasticNet(alpha=1.0, copy_X=True, fit_intercept=True, l1_ratio=0.5,\n",
       "      max_iter=1000, normalize=False, positive=False, precompute=False,\n",
       "      random_state=None, selection='cyclic', tol=0.0001, warm_start=False)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "model= ElasticNet(alpha=1.0, l1_ratio=0.5)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2 Train : 0.712, Test : 0.617\n"
     ]
    }
   ],
   "source": [
    "print('R^2 Train : %.3f, Test : %.3f' % (model.score(X_train, y_train), model.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "alphaと、l1_ratioの値を変えて最適な値を推定しましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha : 0.000010, l1 : 0.100000,  R^2 Train : 0.764, Test : 0.674\n",
      "alpha : 0.000010, l1 : 0.200000,  R^2 Train : 0.764, Test : 0.674\n",
      "alpha : 0.000010, l1 : 0.300000,  R^2 Train : 0.764, Test : 0.674\n",
      "alpha : 0.000010, l1 : 0.400000,  R^2 Train : 0.764, Test : 0.674\n",
      "alpha : 0.000010, l1 : 0.500000,  R^2 Train : 0.764, Test : 0.674\n",
      "alpha : 0.000010, l1 : 0.600000,  R^2 Train : 0.764, Test : 0.674\n",
      "alpha : 0.000010, l1 : 0.700000,  R^2 Train : 0.764, Test : 0.674\n",
      "alpha : 0.000010, l1 : 0.800000,  R^2 Train : 0.764, Test : 0.674\n",
      "alpha : 0.000010, l1 : 0.900000,  R^2 Train : 0.764, Test : 0.674\n",
      "alpha : 0.000100, l1 : 0.100000,  R^2 Train : 0.764, Test : 0.673\n",
      "alpha : 0.000100, l1 : 0.200000,  R^2 Train : 0.764, Test : 0.673\n",
      "alpha : 0.000100, l1 : 0.300000,  R^2 Train : 0.764, Test : 0.673\n",
      "alpha : 0.000100, l1 : 0.400000,  R^2 Train : 0.764, Test : 0.673\n",
      "alpha : 0.000100, l1 : 0.500000,  R^2 Train : 0.764, Test : 0.673\n",
      "alpha : 0.000100, l1 : 0.600000,  R^2 Train : 0.764, Test : 0.673\n",
      "alpha : 0.000100, l1 : 0.700000,  R^2 Train : 0.764, Test : 0.673\n",
      "alpha : 0.000100, l1 : 0.800000,  R^2 Train : 0.764, Test : 0.673\n",
      "alpha : 0.000100, l1 : 0.900000,  R^2 Train : 0.764, Test : 0.673\n",
      "alpha : 0.001000, l1 : 0.100000,  R^2 Train : 0.764, Test : 0.671\n",
      "alpha : 0.001000, l1 : 0.200000,  R^2 Train : 0.764, Test : 0.671\n",
      "alpha : 0.001000, l1 : 0.300000,  R^2 Train : 0.764, Test : 0.671\n",
      "alpha : 0.001000, l1 : 0.400000,  R^2 Train : 0.764, Test : 0.672\n",
      "alpha : 0.001000, l1 : 0.500000,  R^2 Train : 0.764, Test : 0.672\n",
      "alpha : 0.001000, l1 : 0.600000,  R^2 Train : 0.764, Test : 0.672\n",
      "alpha : 0.001000, l1 : 0.700000,  R^2 Train : 0.764, Test : 0.672\n",
      "alpha : 0.001000, l1 : 0.800000,  R^2 Train : 0.764, Test : 0.673\n",
      "alpha : 0.001000, l1 : 0.900000,  R^2 Train : 0.764, Test : 0.673\n",
      "alpha : 0.010000, l1 : 0.100000,  R^2 Train : 0.759, Test : 0.661\n",
      "alpha : 0.010000, l1 : 0.200000,  R^2 Train : 0.759, Test : 0.661\n",
      "alpha : 0.010000, l1 : 0.300000,  R^2 Train : 0.760, Test : 0.662\n",
      "alpha : 0.010000, l1 : 0.400000,  R^2 Train : 0.760, Test : 0.662\n",
      "alpha : 0.010000, l1 : 0.500000,  R^2 Train : 0.760, Test : 0.663\n",
      "alpha : 0.010000, l1 : 0.600000,  R^2 Train : 0.761, Test : 0.664\n",
      "alpha : 0.010000, l1 : 0.700000,  R^2 Train : 0.761, Test : 0.665\n",
      "alpha : 0.010000, l1 : 0.800000,  R^2 Train : 0.762, Test : 0.666\n",
      "alpha : 0.010000, l1 : 0.900000,  R^2 Train : 0.763, Test : 0.668\n",
      "alpha : 0.100000, l1 : 0.100000,  R^2 Train : 0.751, Test : 0.652\n",
      "alpha : 0.100000, l1 : 0.200000,  R^2 Train : 0.751, Test : 0.652\n",
      "alpha : 0.100000, l1 : 0.300000,  R^2 Train : 0.751, Test : 0.652\n",
      "alpha : 0.100000, l1 : 0.400000,  R^2 Train : 0.751, Test : 0.652\n",
      "alpha : 0.100000, l1 : 0.500000,  R^2 Train : 0.752, Test : 0.652\n",
      "alpha : 0.100000, l1 : 0.600000,  R^2 Train : 0.752, Test : 0.653\n",
      "alpha : 0.100000, l1 : 0.700000,  R^2 Train : 0.752, Test : 0.653\n",
      "alpha : 0.100000, l1 : 0.800000,  R^2 Train : 0.753, Test : 0.653\n",
      "alpha : 0.100000, l1 : 0.900000,  R^2 Train : 0.753, Test : 0.653\n",
      "alpha : 1.000000, l1 : 0.100000,  R^2 Train : 0.714, Test : 0.621\n",
      "alpha : 1.000000, l1 : 0.200000,  R^2 Train : 0.714, Test : 0.620\n",
      "alpha : 1.000000, l1 : 0.300000,  R^2 Train : 0.713, Test : 0.619\n",
      "alpha : 1.000000, l1 : 0.400000,  R^2 Train : 0.713, Test : 0.618\n",
      "alpha : 1.000000, l1 : 0.500000,  R^2 Train : 0.712, Test : 0.617\n",
      "alpha : 1.000000, l1 : 0.600000,  R^2 Train : 0.712, Test : 0.616\n",
      "alpha : 1.000000, l1 : 0.700000,  R^2 Train : 0.711, Test : 0.615\n",
      "alpha : 1.000000, l1 : 0.800000,  R^2 Train : 0.710, Test : 0.614\n",
      "alpha : 1.000000, l1 : 0.900000,  R^2 Train : 0.709, Test : 0.613\n",
      "alpha : 10.000000, l1 : 0.100000,  R^2 Train : 0.618, Test : 0.542\n",
      "alpha : 10.000000, l1 : 0.200000,  R^2 Train : 0.601, Test : 0.528\n",
      "alpha : 10.000000, l1 : 0.300000,  R^2 Train : 0.585, Test : 0.514\n",
      "alpha : 10.000000, l1 : 0.400000,  R^2 Train : 0.573, Test : 0.508\n",
      "alpha : 10.000000, l1 : 0.500000,  R^2 Train : 0.561, Test : 0.503\n",
      "alpha : 10.000000, l1 : 0.600000,  R^2 Train : 0.556, Test : 0.499\n",
      "alpha : 10.000000, l1 : 0.700000,  R^2 Train : 0.552, Test : 0.495\n",
      "alpha : 10.000000, l1 : 0.800000,  R^2 Train : 0.547, Test : 0.491\n",
      "alpha : 10.000000, l1 : 0.900000,  R^2 Train : 0.542, Test : 0.486\n",
      "alpha : 100.000000, l1 : 0.100000,  R^2 Train : 0.387, Test : 0.327\n",
      "alpha : 100.000000, l1 : 0.200000,  R^2 Train : 0.343, Test : 0.288\n",
      "alpha : 100.000000, l1 : 0.300000,  R^2 Train : 0.284, Test : 0.233\n",
      "alpha : 100.000000, l1 : 0.400000,  R^2 Train : 0.268, Test : 0.223\n",
      "alpha : 100.000000, l1 : 0.500000,  R^2 Train : 0.250, Test : 0.210\n",
      "alpha : 100.000000, l1 : 0.600000,  R^2 Train : 0.245, Test : 0.207\n",
      "alpha : 100.000000, l1 : 0.700000,  R^2 Train : 0.243, Test : 0.205\n",
      "alpha : 100.000000, l1 : 0.800000,  R^2 Train : 0.240, Test : 0.202\n",
      "alpha : 100.000000, l1 : 0.900000,  R^2 Train : 0.238, Test : 0.199\n",
      "alpha : 1000.000000, l1 : 0.100000,  R^2 Train : 0.233, Test : 0.196\n",
      "alpha : 1000.000000, l1 : 0.200000,  R^2 Train : 0.213, Test : 0.177\n",
      "alpha : 1000.000000, l1 : 0.300000,  R^2 Train : 0.192, Test : 0.161\n",
      "alpha : 1000.000000, l1 : 0.400000,  R^2 Train : 0.164, Test : 0.137\n",
      "alpha : 1000.000000, l1 : 0.500000,  R^2 Train : 0.128, Test : 0.107\n",
      "alpha : 1000.000000, l1 : 0.600000,  R^2 Train : 0.084, Test : 0.068\n",
      "alpha : 1000.000000, l1 : 0.700000,  R^2 Train : 0.032, Test : 0.022\n",
      "alpha : 1000.000000, l1 : 0.800000,  R^2 Train : 0.000, Test : -0.006\n",
      "alpha : 1000.000000, l1 : 0.900000,  R^2 Train : 0.000, Test : -0.006\n",
      "alpha : 10000.000000, l1 : 0.100000,  R^2 Train : 0.000, Test : -0.006\n",
      "alpha : 10000.000000, l1 : 0.200000,  R^2 Train : 0.000, Test : -0.006\n",
      "alpha : 10000.000000, l1 : 0.300000,  R^2 Train : 0.000, Test : -0.006\n",
      "alpha : 10000.000000, l1 : 0.400000,  R^2 Train : 0.000, Test : -0.006\n",
      "alpha : 10000.000000, l1 : 0.500000,  R^2 Train : 0.000, Test : -0.006\n",
      "alpha : 10000.000000, l1 : 0.600000,  R^2 Train : 0.000, Test : -0.006\n",
      "alpha : 10000.000000, l1 : 0.700000,  R^2 Train : 0.000, Test : -0.006\n",
      "alpha : 10000.000000, l1 : 0.800000,  R^2 Train : 0.000, Test : -0.006\n",
      "alpha : 10000.000000, l1 : 0.900000,  R^2 Train : 0.000, Test : -0.006\n"
     ]
    }
   ],
   "source": [
    "for i in range(-5, 5):\n",
    "    for j in range(1,10):\n",
    "        al = 10 ** i\n",
    "        l1 = j / 10\n",
    "        model= ElasticNet(alpha=al, l1_ratio=l1)\n",
    "        model.fit(X_train, y_train)\n",
    "        print('alpha : %f, l1 : %f,  R^2 Train : %.3f, Test : %.3f' % (al, l1,  model.score(X_train, y_train), model.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最高結果は、\n",
    "\n",
    "~~~\n",
    "R^2 Train : 0.764, Test : 0.674\n",
    "~~~\n",
    "となりました。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "どうやら、このデータセットでは正則化しない方が精度がよさげですね。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次は、非線形関係をモデリングすることに関して勉強していきます。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
